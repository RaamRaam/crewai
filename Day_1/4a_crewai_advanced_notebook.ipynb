{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrewAI Workflows Advanced\n",
    "\n",
    "This notebook covers advanced patterns and production deployment for CrewAI Workflows.\n",
    "\n",
    "## Topics Covered\n",
    "- Advanced agent patterns\n",
    "- Complex workflow orchestration\n",
    "- Production monitoring\n",
    "- External system integration\n",
    "- Enterprise deployment strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced setup for CrewAI Workflows\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import asyncio\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai_tools import SerperDevTool, FileReadTool, DirectoryReadTool\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Advanced CrewAI setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class WorkflowConfig:\n",
    "    name: str\n",
    "    max_iterations: int = 5\n",
    "    timeout: int = 300\n",
    "    retry_attempts: int = 3\n",
    "    enable_monitoring: bool = True\n",
    "\n",
    "config = WorkflowConfig(\n",
    "    name=\"Advanced Pipeline\",\n",
    "    max_iterations=10,\n",
    "    timeout=600\n",
    ")\n",
    "\n",
    "print(f\"Configuration: {config.name}\")\n",
    "print(f\"Max iterations: {config.max_iterations}\")\n",
    "print(f\"Timeout: {config.timeout}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Agent Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedAgentFactory:\n",
    "    @staticmethod\n",
    "    def create_supervisor(config: WorkflowConfig) -> Agent:\n",
    "        return Agent(\n",
    "            role=\"Workflow Supervisor\",\n",
    "            goal=\"Orchestrate and monitor workflow execution\",\n",
    "            backstory=\"Experienced supervisor coordinating multiple agents\",\n",
    "            verbose=True,\n",
    "            allow_delegation=True,\n",
    "            max_iter=config.max_iterations,\n",
    "            tools=[SerperDevTool()]\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_specialist(specialty: str) -> Agent:\n",
    "        return Agent(\n",
    "            role=f\"{specialty.title()} Specialist\",\n",
    "            goal=f\"Provide expert {specialty} analysis\",\n",
    "            backstory=f\"Domain expert in {specialty}\",\n",
    "            verbose=True,\n",
    "            tools=[SerperDevTool()]\n",
    "        )\n",
    "\n",
    "# Create specialized agents\n",
    "factory = AdvancedAgentFactory()\n",
    "supervisor = factory.create_supervisor(config)\n",
    "security_agent = factory.create_specialist(\"security\")\n",
    "compliance_agent = factory.create_specialist(\"compliance\")\n",
    "\n",
    "print(\"Advanced agents created:\")\n",
    "print(f\"- {supervisor.role}\")\n",
    "print(f\"- {security_agent.role}\")\n",
    "print(f\"- {compliance_agent.role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Workflow Pipeline with Error Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustPipeline:\n",
    "    def __init__(self, config: WorkflowConfig):\n",
    "        self.config = config\n",
    "        self.execution_history = []\n",
    "        self.error_log = []\n",
    "    \n",
    "    def create_stages(self):\n",
    "        # Stage 1: Data Collection\n",
    "        collector = Agent(\n",
    "            role=\"Data Collector\",\n",
    "            goal=\"Gather data from multiple sources\",\n",
    "            backstory=\"Expert in data collection\",\n",
    "            tools=[SerperDevTool(), FileReadTool()],\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        collect_task = Task(\n",
    "            description=\"Collect data on {topic}\",\n",
    "            expected_output=\"Structured dataset with sources\",\n",
    "            agent=collector\n",
    "        )\n",
    "        \n",
    "        # Stage 2: Processing\n",
    "        processor = Agent(\n",
    "            role=\"Data Processor\",\n",
    "            goal=\"Process and structure information\",\n",
    "            backstory=\"Specialist in data processing\",\n",
    "            tools=[FileReadTool()],\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        process_task = Task(\n",
    "            description=\"Process collected data\",\n",
    "            expected_output=\"Processed information with insights\",\n",
    "            agent=processor\n",
    "        )\n",
    "        \n",
    "        stage1 = Crew([collector], [collect_task], Process.sequential)\n",
    "        stage2 = Crew([processor], [process_task], Process.sequential)\n",
    "        \n",
    "        return [stage1, stage2]\n",
    "    \n",
    "    def execute_with_retry(self, crew, inputs, stage_name):\n",
    "        for attempt in range(self.config.retry_attempts):\n",
    "            try:\n",
    "                logger.info(f\"Executing {stage_name}, attempt {attempt + 1}\")\n",
    "                \n",
    "                # Demo execution\n",
    "                result = f\"Demo result for {stage_name}\"\n",
    "                \n",
    "                self.execution_history.append({\n",
    "                    'stage': stage_name,\n",
    "                    'attempt': attempt + 1,\n",
    "                    'success': True,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.error_log.append({\n",
    "                    'stage': stage_name,\n",
    "                    'attempt': attempt + 1,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "                \n",
    "                if attempt < self.config.retry_attempts - 1:\n",
    "                    time.sleep(2)\n",
    "                else:\n",
    "                    return None\n",
    "\n",
    "pipeline = RobustPipeline(config)\n",
    "stages = pipeline.create_stages()\n",
    "\n",
    "print(f\"Pipeline created with {len(stages)} stages\")\n",
    "print(\"Features: Error recovery, retry logic, execution tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Monitoring System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkflowMonitor:\n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'executions': 0,\n",
    "            'successes': 0,\n",
    "            'failures': 0,\n",
    "            'total_time': 0\n",
    "        }\n",
    "        self.alerts = []\n",
    "    \n",
    "    def start_monitoring(self, workflow_id: str) -> str:\n",
    "        execution_id = f\"{workflow_id}_{int(time.time())}\"\n",
    "        self.metrics['executions'] += 1\n",
    "        logger.info(f\"Started monitoring: {execution_id}\")\n",
    "        return execution_id\n",
    "    \n",
    "    def log_activity(self, execution_id: str, agent_id: str, activity: str):\n",
    "        log_entry = {\n",
    "            'execution_id': execution_id,\n",
    "            'agent_id': agent_id,\n",
    "            'activity': activity,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        logger.info(f\"Activity logged: {agent_id} - {activity}\")\n",
    "    \n",
    "    def complete_monitoring(self, execution_id: str, success: bool, duration: float):\n",
    "        if success:\n",
    "            self.metrics['successes'] += 1\n",
    "        else:\n",
    "            self.metrics['failures'] += 1\n",
    "        \n",
    "        self.metrics['total_time'] += duration\n",
    "        \n",
    "        success_rate = self.metrics['successes'] / self.metrics['executions']\n",
    "        avg_time = self.metrics['total_time'] / self.metrics['executions']\n",
    "        \n",
    "        logger.info(f\"Execution complete: {execution_id}\")\n",
    "        logger.info(f\"Success rate: {success_rate:.2%}\")\n",
    "        logger.info(f\"Average time: {avg_time:.2f}s\")\n",
    "    \n",
    "    def get_report(self):\n",
    "        success_rate = self.metrics['successes'] / max(self.metrics['executions'], 1)\n",
    "        avg_time = self.metrics['total_time'] / max(self.metrics['executions'], 1)\n",
    "        \n",
    "        return {\n",
    "            'total_executions': self.metrics['executions'],\n",
    "            'success_rate': success_rate,\n",
    "            'average_time': avg_time,\n",
    "            'generated_at': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "monitor = WorkflowMonitor()\n",
    "execution_id = monitor.start_monitoring(\"demo_workflow\")\n",
    "monitor.log_activity(execution_id, \"agent_01\", \"data_collection\")\n",
    "monitor.complete_monitoring(execution_id, True, 45.2)\n",
    "\n",
    "report = monitor.get_report()\n",
    "print(f\"Monitoring report: {report}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. External System Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SystemIntegrator:\n",
    "    def __init__(self):\n",
    "        self.integrations = {}\n",
    "        self.circuit_breakers = {}\n",
    "    \n",
    "    def register_system(self, name: str, config: dict):\n",
    "        self.integrations[name] = {\n",
    "            'config': config,\n",
    "            'status': 'active',\n",
    "            'errors': 0,\n",
    "            'successes': 0\n",
    "        }\n",
    "        \n",
    "        self.circuit_breakers[name] = {\n",
    "            'state': 'closed',\n",
    "            'failures': 0,\n",
    "            'threshold': config.get('failure_threshold', 5)\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Registered system: {name}\")\n",
    "    \n",
    "    async def call_system(self, system_name: str, operation: str, data: dict):\n",
    "        if system_name not in self.integrations:\n",
    "            raise ValueError(f\"System {system_name} not registered\")\n",
    "        \n",
    "        try:\n",
    "            # Simulate API call\n",
    "            await asyncio.sleep(0.1)\n",
    "            \n",
    "            result = {\n",
    "                'system': system_name,\n",
    "                'operation': operation,\n",
    "                'status': 'success',\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            self.integrations[system_name]['successes'] += 1\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.integrations[system_name]['errors'] += 1\n",
    "            logger.error(f\"System call failed: {system_name}.{operation}\")\n",
    "            raise\n",
    "\n",
    "# Setup integrations\n",
    "integrator = SystemIntegrator()\n",
    "integrator.register_system('salesforce', {'timeout': 30, 'failure_threshold': 3})\n",
    "integrator.register_system('slack', {'timeout': 10, 'failure_threshold': 2})\n",
    "\n",
    "print(\"External system integrations configured:\")\n",
    "for system in integrator.integrations:\n",
    "    print(f\"- {system}: active\")\n",
    "\n",
    "print(\"\\nFeatures:\")\n",
    "print(\"✓ Circuit breaker pattern\")\n",
    "print(\"✓ Failure tracking\")\n",
    "print(\"✓ Configurable thresholds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Production Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production deployment configuration\n",
    "deployment_config = {\n",
    "    'containerization': {\n",
    "        'base_image': 'python:3.11-slim',\n",
    "        'features': ['health_checks', 'non_root_user', 'security_scanning']\n",
    "    },\n",
    "    'services': {\n",
    "        'api_server': 'FastAPI application',\n",
    "        'redis': 'Caching and queuing',\n",
    "        'postgres': 'Data persistence',\n",
    "        'monitoring': 'Prometheus metrics'\n",
    "    },\n",
    "    'scaling': {\n",
    "        'horizontal': 'Multiple API instances',\n",
    "        'auto_scaling': 'CPU and memory based',\n",
    "        'load_balancing': 'Traffic distribution'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Production Deployment Configuration:\")\n",
    "print(f\"Base image: {deployment_config['containerization']['base_image']}\")\n",
    "print(f\"Services: {len(deployment_config['services'])}\")\n",
    "print(f\"Scaling options: {len(deployment_config['scaling'])}\")\n",
    "\n",
    "print(\"\\nProduction Features:\")\n",
    "print(\"✓ Docker containerization\")\n",
    "print(\"✓ Multi-service architecture\")\n",
    "print(\"✓ Health monitoring\")\n",
    "print(\"✓ Auto-scaling\")\n",
    "print(\"✓ Load balancing\")\n",
    "print(\"✓ Security hardening\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionAPI:\n",
    "    def __init__(self):\n",
    "        self.endpoints = {\n",
    "            'execute': '/workflows/execute',\n",
    "            'status': '/workflows/{id}/status',\n",
    "            'list': '/workflows',\n",
    "            'cancel': '/workflows/{id}',\n",
    "            'health': '/health',\n",
    "            'metrics': '/metrics'\n",
    "        }\n",
    "        \n",
    "        self.features = [\n",
    "            'async_execution',\n",
    "            'real_time_status',\n",
    "            'workflow_cancellation',\n",
    "            'health_monitoring',\n",
    "            'prometheus_metrics'\n",
    "        ]\n",
    "    \n",
    "    def get_api_info(self):\n",
    "        return {\n",
    "            'title': 'CrewAI Workflows API',\n",
    "            'version': '1.0.0',\n",
    "            'endpoints': len(self.endpoints),\n",
    "            'features': len(self.features)\n",
    "        }\n",
    "\n",
    "api = ProductionAPI()\n",
    "info = api.get_api_info()\n",
    "\n",
    "print(f\"API: {info['title']} v{info['version']}\")\n",
    "print(f\"Endpoints: {info['endpoints']}\")\n",
    "print(f\"Features: {info['features']}\")\n",
    "\n",
    "print(\"\\nAPI Endpoints:\")\n",
    "for name, endpoint in api.endpoints.items():\n",
    "    print(f\"- {name.upper()}: {endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Enterprise Workflow Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnterpriseOrchestrator:\n",
    "    def __init__(self, config: WorkflowConfig):\n",
    "        self.config = config\n",
    "        self.monitor = WorkflowMonitor()\n",
    "        self.integrator = SystemIntegrator()\n",
    "        \n",
    "        # Setup enterprise systems\n",
    "        self.integrator.register_system('salesforce', {'timeout': 30})\n",
    "        self.integrator.register_system('slack', {'timeout': 10})\n",
    "        self.integrator.register_system('jira', {'timeout': 20})\n",
    "    \n",
    "    def create_enterprise_agents(self):\n",
    "        agents = {\n",
    "            'data_analyst': Agent(\n",
    "                role=\"Senior Data Analyst\",\n",
    "                goal=\"Analyze enterprise data and generate insights\",\n",
    "                backstory=\"Expert in enterprise data analysis\",\n",
    "                tools=[SerperDevTool(), FileReadTool()],\n",
    "                verbose=True\n",
    "            ),\n",
    "            'compliance_officer': Agent(\n",
    "                role=\"Compliance Officer\",\n",
    "                goal=\"Ensure regulatory compliance\",\n",
    "                backstory=\"Expert in regulatory requirements\",\n",
    "                tools=[SerperDevTool()],\n",
    "                verbose=True\n",
    "            ),\n",
    "            'qa_manager': Agent(\n",
    "                role=\"Quality Assurance Manager\",\n",
    "                goal=\"Validate quality standards\",\n",
    "                backstory=\"Expert in quality management\",\n",
    "                tools=[FileReadTool()],\n",
    "                verbose=True\n",
    "            )\n",
    "        }\n",
    "        return agents\n",
    "    \n",
    "    async def execute_enterprise_workflow(self, objective: str):\n",
    "        execution_id = self.monitor.start_monitoring('enterprise_001')\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Create agents\n",
    "            agents = self.create_enterprise_agents()\n",
    "            \n",
    "            # Execute workflow stages\n",
    "            stages = [\n",
    "                ('data_extraction', 'Extract enterprise data'),\n",
    "                ('compliance_review', 'Review compliance'),\n",
    "                ('quality_validation', 'Validate quality')\n",
    "            ]\n",
    "            \n",
    "            results = {}\n",
    "            \n",
    "            for stage_name, description in stages:\n",
    "                self.monitor.log_activity(execution_id, stage_name, 'processing')\n",
    "                \n",
    "                # Simulate processing\n",
    "                await asyncio.sleep(0.5)\n",
    "                \n",
    "                results[stage_name] = f\"Completed {description} for {objective}\"\n",
    "            \n",
    "            duration = time.time() - start_time\n",
    "            self.monitor.complete_monitoring(execution_id, True, duration)\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'execution_id': execution_id,\n",
    "                'results': results,\n",
    "                'duration': duration\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Enterprise workflow failed: {e}\")\n",
    "            raise\n",
    "\n",
    "# Initialize enterprise orchestrator\n",
    "enterprise_config = WorkflowConfig(\n",
    "    name=\"Enterprise Data Pipeline\",\n",
    "    max_iterations=8,\n",
    "    timeout=1200\n",
    ")\n",
    "\n",
    "orchestrator = EnterpriseOrchestrator(enterprise_config)\n",
    "\n",
    "print(\"Enterprise Workflow Orchestrator Initialized\")\n",
    "print(f\"Configuration: {enterprise_config.name}\")\n",
    "print(f\"Integrations: {len(orchestrator.integrator.integrations)}\")\n",
    "print(\"\\nCapabilities:\")\n",
    "print(\"✓ Multi-agent coordination\")\n",
    "print(\"✓ External system integration\")\n",
    "print(\"✓ Real-time monitoring\")\n",
    "print(\"✓ Enterprise compliance\")\n",
    "print(\"✓ Quality assurance\")\n",
    "\n",
    "# Demo execution would be:\n",
    "# result = await orchestrator.execute_enterprise_workflow(\"Q4 Revenue Analysis\")\n",
    "print(\"\\nTo execute: await orchestrator.execute_enterprise_workflow('Business Objective')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Production Checklist and Best Practices\n",
    "\n",
    "### Production Deployment Checklist\n",
    "\n",
    "#### Infrastructure ✅\n",
    "- **Containerization**: Docker images with security scanning\n",
    "- **Orchestration**: Kubernetes or Docker Compose\n",
    "- **Load Balancing**: Horizontal scaling capabilities\n",
    "- **Service Discovery**: Dynamic service registration\n",
    "\n",
    "#### Monitoring & Observability ✅\n",
    "- **Metrics**: Prometheus integration\n",
    "- **Logging**: Structured logging with correlation IDs\n",
    "- **Alerting**: Real-time alerts for critical issues\n",
    "- **Dashboards**: Grafana visualization\n",
    "\n",
    "#### Security ✅\n",
    "- **Authentication**: API key or OAuth integration\n",
    "- **Authorization**: Role-based access control\n",
    "- **Secrets Management**: Secure credential storage\n",
    "- **Network Security**: TLS encryption\n",
    "\n",
    "#### Reliability ✅\n",
    "- **Error Handling**: Circuit breakers and retry logic\n",
    "- **Health Checks**: Comprehensive monitoring\n",
    "- **Backup & Recovery**: Data persistence strategies\n",
    "- **Disaster Recovery**: Multi-region deployment\n",
    "\n",
    "### Performance Optimization ✅\n",
    "- **Caching**: Redis for fast data access\n",
    "- **Async Processing**: Background task execution\n",
    "- **Resource Optimization**: Memory and CPU efficiency\n",
    "- **Auto-scaling**: Load-based scaling\n",
    "\n",
    "### Next Steps\n",
    "1. Set up CI/CD pipeline for automated deployments\n",
    "2. Implement comprehensive testing suite\n",
    "3. Configure monitoring stack (Prometheus, Grafana, ELK)\n",
    "4. Establish incident response procedures\n",
    "5. Create operational runbooks\n",
    "\n",
    "### Resources\n",
    "- [CrewAI Documentation](https://docs.crewai.com/)\n",
    "- [12-Factor App Methodology](https://12factor.net/)\n",
    "- [Kubernetes Best Practices](https://kubernetes.io/docs/concepts/)\n",
    "- [Site Reliability Engineering](https://sre.google/sre-book/)\n",
    "\n",
    "**Congratulations!** You've mastered advanced CrewAI Workflows and are ready for enterprise production deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}